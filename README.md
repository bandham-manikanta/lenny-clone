# Documentation Package

Let me create natural, practical documentation that gets straight to the point.

---

## **README.md**

```markdown
# LennyBot ğŸ§¢

An AI chatbot that thinks and talks like Lenny Rachitsky. Built with RAG, NVIDIA NIM, and ChromaDB.

Ask it about product-market fit, retention benchmarks, or hiring your first PMâ€”and get answers that sound like they came straight from Lenny's newsletter.

---

## What Makes This Different

**Dual-Stream RAG:** Separates Lenny's opinions (from LinkedIn) from guest insights (from YouTube transcripts). This means when you ask about PMF, you get Lenny's framework, not a mashup of what 50 different guests said.

**Conversational Persona:** Uses a style system that mimics Lenny's writing voiceâ€”short paragraphs, tactical advice, no corporate fluff.

**Streaming Responses:** Powered by NVIDIA NIM's Llama 3.1 70B with token-by-token streaming for that ChatGPT feel.

---

## Quick Start

```bash
# Clone and install
git clone <your-repo>
cd lenny-clone
uv pip install -r requirements.txt

# Set up environment
cp .env.example .env
# Add your API keys (see below)

# Load the vector database
uv run python ingestion/load_chroma.py

# Run the app
uv run streamlit run app/streamlit_app.py
```

Open http://localhost:8501 and start chatting.

---

## Environment Variables

Create a `.env` file:

```bash
# NVIDIA NIM (Required)
NVIDIA_API_KEY=nvapi-xxx
NVIDIA_BASE_URL=https://integrate.api.nvidia.com/v1

# Data Collection (Optional - only needed for ingestion)
YOUTUBE_API_KEY=your-key-here
APIFY_API_KEY=apify-xxx

# Vector DB
CHROMA_COLLECTION_NAME=lenny_clone
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
```

**Get NVIDIA API Key:** https://build.nvidia.com/  
**Get YouTube API Key:** https://console.cloud.google.com/apis/credentials

---

## Project Structure

```
lenny-clone/
â”œâ”€â”€ agent/                      # Core RAG system
â”‚   â”œâ”€â”€ rag.py                 # Main query pipeline
â”‚   â”œâ”€â”€ persona.py             # Lenny's voice & frameworks
â”‚   â”œâ”€â”€ llm_client.py          # NVIDIA NIM interface
â”‚   â””â”€â”€ chroma_retriever.py    # Vector search
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py       # Chat UI
â”‚
â”œâ”€â”€ ingestion/                  # Data pipeline
â”‚   â”œâ”€â”€ extract_linkedin.py    # Scrape posts
â”‚   â”œâ”€â”€ extract_youtube.py     # Get transcripts
â”‚   â””â”€â”€ load_chroma.py         # Build vector DB
â”‚
â”œâ”€â”€ evaluation/                 # Quality metrics
â”‚   â””â”€â”€ eval.py                # LLM-as-a-judge tests
â”‚
â””â”€â”€ data/                       # Raw data (gitignored)
```

---

## How It Works

**1. Ingestion**
- Scrapes Lenny's LinkedIn posts (his direct voice)
- Downloads YouTube transcripts (separates Lenny from guests)
- Chunks text and embeds with sentence-transformers
- Stores in ChromaDB with metadata tags (`linkedin` vs `youtube`)

**2. Retrieval**
When you ask a question:
- Gets top 3 chunks from LinkedIn (Lenny's opinions)
- Gets top 3 chunks from YouTube (guest examples)
- Ranks by semantic similarity

**3. Generation**
- Builds a prompt with Lenny's core frameworks (PMF, retention, hiring)
- Adds retrieved context
- Streams response from Llama 3.1 70B
- Style: conversational, tactical, no fluff

**4. UI**
- Streamlit chat interface
- Adjustable creativity (temperature) and sources (top-k)
- Shows source citations with match scores

---

## Usage Examples

**Ask about frameworks:**
```
What is product-market fit?
â†’ Gets PMF definition + retention curve benchmark
```

**Ask for benchmarks:**
```
What's a good retention rate for B2B SaaS?
â†’ "SMB: 60% annual, Enterprise: 80%+"
```

**Ask for advice:**
```
Should I hire a PM before finding PMF?
â†’ Lenny-style contrarian take
```

---

## Development

**Run tests:**
```bash
uv run python test_persona.py
```

**Evaluate quality:**
```bash
uv run python evaluation/eval.py
```

**Re-ingest data:**
```bash
# Extract fresh data
uv run python ingestion/extract_linkedin.py
uv run python ingestion/extract_youtube.py

# Rebuild vector DB
uv run python ingestion/load_chroma.py
```

---

## Tech Stack

- **LLM:** NVIDIA NIM (Llama 3.1 70B)
- **Vector DB:** ChromaDB (local, persistent)
- **Embeddings:** sentence-transformers/all-MiniLM-L6-v2
- **UI:** Streamlit
- **Data:** Apify (LinkedIn), YouTube Data API v3

---

## Limitations

- **Not real-time:** Uses scraped data, not live content
- **Context window:** Limited to top-k retrieved chunks
- **Voice accuracy:** Mimics style but isn't Lenny
- **Data freshness:** Needs manual re-scraping

---

## Deployment

See [DEPLOYMENT.md](DEPLOYMENT.md) for Vercel setup.

---

## License

MIT

---

Built with ğŸ§¢ by [Your Name]
```

---

## **DEPLOYMENT.md**

```markdown
# Deploying to Vercel

LennyBot runs on Vercel with serverless functions. Here's how to deploy it.

---

## Prerequisites

- Vercel account (free tier works)
- GitHub repo with your code
- NVIDIA API key

---

## Setup Steps

### 1. Connect GitHub to Vercel

```bash
# Push your code
git add .
git commit -m "Ready for deployment"
git push origin main
```

Go to https://vercel.com/new and import your repo.

### 2. Configure Build Settings

**Framework Preset:** Other  
**Build Command:**
```bash
uv pip install -r requirements.txt && python ingestion/load_chroma.py
```

**Output Directory:** `app`

**Install Command:**
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh && uv pip install -r requirements.txt
```

### 3. Add Environment Variables

In Vercel dashboard â†’ Settings â†’ Environment Variables:

```
NVIDIA_API_KEY=nvapi-xxx
NVIDIA_BASE_URL=https://integrate.api.nvidia.com/v1
CHROMA_COLLECTION_NAME=lenny_clone
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
```

### 4. Deploy

Click "Deploy" and wait ~3 minutes.

---

## Post-Deployment

**Custom Domain (Optional):**
Settings â†’ Domains â†’ Add `lennyclone.yourdomain.com`

**Update Data:**
Re-run ingestion locally, commit `chroma_db/` folder, and push.

---

## Troubleshooting

**Build fails on ChromaDB:**
Add to `requirements.txt`:
```
chromadb==0.4.22
pysqlite3-binary
```

**Streamlit won't start:**
Check logs for port conflicts. Vercel expects port 8501.

**Slow cold starts:**
Vercel serverless has ~5s cold start. Consider Vercel Pro for faster spin-up.

---

## Monitoring

- **Logs:** Vercel dashboard â†’ Deployments â†’ View Function Logs
- **Analytics:** Enable Web Analytics in Vercel settings
- **Cost:** Free tier = 100GB-hrs/month (should be plenty)

---

## Alternative: Docker Deployment

If you want to deploy elsewhere (Railway, Render, Fly.io):

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY . .

RUN pip install uv && uv pip install -r requirements.txt
RUN python ingestion/load_chroma.py

EXPOSE 8501
CMD ["streamlit", "run", "app/streamlit_app.py", "--server.port=8501"]
```

---

Need help? Open an issue on GitHub.
```

---

## **ARCHITECTURE.md**

```markdown
# Architecture

How LennyBot works under the hood.

---

## System Overview

```
User Question
    â†“
[Streamlit UI] â† app/streamlit_app.py
    â†“
[RAG Pipeline] â† agent/rag.py
    â†“
    â”œâ”€â†’ [Retriever] â† agent/chroma_retriever.py
    â”‚       â†“
    â”‚   [ChromaDB] â† Vector search (dual-stream)
    â”‚       â†“
    â”‚   Returns: Lenny chunks + Guest chunks
    â”‚
    â”œâ”€â†’ [Persona] â† agent/persona.py
    â”‚       â†“
    â”‚   Adds: Core frameworks (PMF, retention, hiring)
    â”‚       â†“
    â”‚   Builds: Enhanced prompt
    â”‚
    â””â”€â†’ [LLM Client] â† agent/llm_client.py
            â†“
        [NVIDIA NIM] â† Llama 3.1 70B
            â†“
        Streams response token-by-token
```

---

## Key Components

### 1. Dual-Stream Retrieval

**Why?** Lenny's opinions â‰  Guest insights.

**How it works:**
```python
# Query ChromaDB twice
lenny_chunks = search(query, source_filter="linkedin")  # His posts
guest_chunks = search(query, source_filter="youtube")   # Podcast transcripts

# Combine with weights
context = lenny_chunks[:3] + guest_chunks[:3]
```

This prevents guest advice from being attributed to Lenny.

### 2. Persona System

**Core Frameworks:** Hardcoded beliefs (PMF, retention benchmarks, hiring criteria)

**Style Guide:**
- Use contractions ("you're", "don't")
- Start with "Look" or "Here's the thing"
- Short paragraphs (1-3 sentences)
- Ask rhetorical questions

**Prompt Structure:**
```
Question: {user_question}

What you know: {core_framework}

From your writing: {lenny_chunks}
From guests: {guest_chunks}

Write 2-3 paragraphs. Sound like you're talking, not writing an essay.
```

### 3. Streaming Pipeline

```python
def query(question, stream=True):
    # Retrieve
    chunks = retriever.search(question)
    
    # Build prompt
    prompt = persona.get_enhanced_prompt(question, chunks)
    
    # Stream LLM response
    response = llm.generate(prompt, stream=True)
    
    # Yield chunks
    for chunk in response:
        yield chunk
```

Streamlit displays each chunk immediately for that ChatGPT feel.

---

## Data Flow

### Ingestion Pipeline

```
LinkedIn Posts â†’ extract_linkedin.py
                    â†“
                Clean text
                    â†“
YouTube Videos â†’ extract_youtube.py â†’ Clean transcripts
                    â†“                      â†“
                Separate Lenny vs Guest voices
                    â†“
                Chunk (500 chars, 50 overlap)
                    â†“
                Embed (sentence-transformers)
                    â†“
                ChromaDB (persistent, local)
```

### Query Pipeline

```
User: "What is PMF?"
    â†“
Embed query â†’ [0.23, -0.45, 0.12, ...]
    â†“
ChromaDB cosine similarity search
    â†“
Top 6 chunks (3 linkedin + 3 youtube)
    â†“
Add core framework for "PMF"
    â†“
Build prompt
    â†“
NVIDIA NIM (Llama 3.1 70B)
    â†“
Stream response
    â†“
Display with sources
```

---

## Design Decisions

**Why ChromaDB over Pinecone?**  
Local-first. No API costs, faster for small datasets (<10k chunks).

**Why sentence-transformers over OpenAI embeddings?**  
Free, fast, good enough for semantic search. OpenAI would cost $0.0001/1k tokens.

**Why dual-stream retrieval?**  
Prevents hallucinations. If Lenny never said it, don't attribute it to him.

**Why hardcode core frameworks?**  
Some beliefs are fundamental. Better to explicitly encode them than hope the LLM infers them.

**Why NVIDIA NIM over OpenAI?**  
Cheaper ($0.001/1k tokens vs $0.03/1k), faster streaming, open weights (Llama).

---

## Performance

**Typical query:**
- Embedding: ~0.05s
- Vector search: ~0.1s
- LLM generation: ~2s (streaming)
- **Total:** ~2.2s to first token

**Optimization tips:**
- Cache embeddings (âœ… already implemented)
- Use smaller embedding model if too slow
- Reduce top-k if retrieval is slow

---

## Evaluation Metrics

See `evaluation/eval.py` for:
- **Faithfulness:** Is response grounded in context?
- **Style Match:** Does it sound like Lenny?
- **Accuracy:** Does it answer the question?

Uses LLM-as-a-Judge (same Llama 3.1 70B model scores responses 1-5).

---

## Future Improvements

**Multi-turn conversations:**  
Add memory to track conversation history.

**Fine-tuned model:**  
Train Llama on Lenny's writing for better voice match.

**Real-time updates:**  
Auto-scrape LinkedIn weekly and rebuild index.

**Citations:**  
Link to specific posts/videos in sources.
